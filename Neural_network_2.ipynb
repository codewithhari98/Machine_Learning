{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural-network-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeNDJPCABq33pboXQc4EyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codewithhari98/Machine_Learning/blob/main/Neural_network_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8fgBIiO-50W9"
      },
      "outputs": [],
      "source": [
        "# Idea1: create a global w and b of size L which is number of layers\n",
        "#https://www.youtube.com/watch?v=RSl87lqOXDE&ab_channel=CoreySchafer\n",
        "#https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce654\n",
        "#https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
        "#http://neuralnetworksanddeeplearning.com/chap1.html\n",
        "#https://www.educative.io/answers/one-hot-encoding-in-python\n",
        "#https://e2eml.school/softmax.html\n",
        "import numpy as np\n",
        "class Layer:\n",
        "  def __init__(self,inputsize,outputsize):\n",
        "    self.input=inputsize\n",
        "    self.output=outputsize\n",
        "    self.weights = np.random.rand(inputsize, outputsize) - 0.5\n",
        "    self.bias = np.random.rand(1, outputsize) - 0.5\n",
        "  def forward_propagation(self, input):\n",
        "    return -1\n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    return -1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class linearlayer(Layer):\n",
        "  def forward_propagation(self,input):\n",
        "    self.input_data=input\n",
        "    f=np.dot(self.input_data, self.weights) + self.bias\n",
        "    return f\n",
        "  def backward_propagation(self,learning_rate,output_error):\n",
        "    #diff of function f wrt weights gives input data \n",
        "    dfw=self.input_data\n",
        "    dfx=self.weights\n",
        "    #calculating sensitivity of cost wrt previous activation layer\n",
        "    input_error = np.dot(output_error, dfx.T)\n",
        "    #calculating sensitivity of cost wrt change in weights\n",
        "    weights_error = np.dot(dfw.T, output_error)\n",
        "\n",
        "    self.weights=self.weights-learning_rate*weights_error\n",
        "    self.bias=self.bias-learning_rate*output_error\n",
        "    #this returns the sensitivity of the cost func wrt previous activation func\n",
        "    #which will be useful to calc the change in weights needed at inp\n",
        "    return input_error"
      ],
      "metadata": {
        "id": "eFII3yuofpYP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sigmoid(Layer):\n",
        "  def forward_propagation(self,input):\n",
        "    self.input_data=input\n",
        "    self.z=(np.dot(self.input_data, self.weights) + self.bias)\n",
        "    return 1. / (1. + np.exp(-self.z))\n",
        "  def backward_propagation(self, output_error, learning_rate):\n",
        "    sig_prime= (1. / (1. + np.exp(-self.z)))*(1- (1. / (1. + np.exp(-self.z))))*output_error*self.weights.T \n",
        "    return sig_prime\n",
        "\n"
      ],
      "metadata": {
        "id": "1gg_ggjwhsvW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tanh(Layer):\n",
        "  def forward_propagation(self,input):\n",
        "    self.input_data=input\n",
        "    self.z=(np.dot(self.input_data, self.weights) + self.bias)\n",
        "    tanh_val=(np.exp(self.z) - np.exp(-self.z)) / (np.exp(self.z) + np.exp(-self.z))\n",
        "    return tanh_val\n",
        "  def backward_propagation(self, output_error):\n",
        "    tanh_val=(np.exp(self.z) - np.exp(-self.z)) / (np.exp(self.z) + np.exp(-self.z))\n",
        "    tanh_prime= (1-(tanh_val)**2)*output_error \n",
        "    return tanh_prime"
      ],
      "metadata": {
        "id": "XMTEwNYu7cSd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class softmax(Layer):\n",
        "  def forward_propagation(self,input):\n",
        "    self.input_data=input\n",
        "    self.z=(np.dot(self.input_data, self.weights) + self.bias)\n",
        "    num = np.exp(self.z- np.max(self.z))\n",
        "    return num / np.sum(num, axis=0, keepdims=True)\n",
        "    \n",
        "  #*****************************************************#\n",
        "  def backward_propagation(self, probs, bp_err):\n",
        "    dim = probs.shape[1]\n",
        "    output = np.empty(probs.shape)\n",
        "    for j in range(dim):\n",
        "        d_prob_over_xj = - (probs * probs[:,[j]])  # i.e. prob_k * prob_j, no matter k==j or not\n",
        "        d_prob_over_xj[:,j] += probs[:,j]   # i.e. when k==j, +prob_j\n",
        "        output[:,j] = np.sum(bp_err * d_prob_over_xj, axis=1)\n",
        "    return output"
      ],
      "metadata": {
        "id": "f2wF_gHR7dRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class crossentropy(Layer):\n",
        "  def forward_propagation(pred, target):\n",
        "    return -target * np.log(pred)\n",
        "  def backward_propagation(pred, target):\n",
        "    return target - pred"
      ],
      "metadata": {
        "id": "T8WdnawA7iqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to calculate the loss function and its derivative\n",
        "class output_error:\n",
        "  def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "  def mse_prime(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "-B3wWORTszhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sequential_class(Layer):\n",
        "  def __init__(self,loss=None,loss_grad=None):\n",
        "    self.layers = []\n",
        "    self.loss=loss\n",
        "    self.loss_grad=loss_grad\n",
        "  def add_layer(self,newlayer):\n",
        "    self.layers.append(newlayer)\n",
        "  def predict(self,input):\n",
        "    samplesize=len(input)\n",
        "    predictions=[]\n",
        "    for i in range(samplesize):\n",
        "      output=input[i]\n",
        "      for layer in self.layers:\n",
        "        output = layer.forward_propagation(output)\n",
        "        predictions.append(output)\n",
        "    return predictions\n",
        "  def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "    samplesize=len(x_train)\n",
        "    for i in range(epochs):\n",
        "      errorval=0\n",
        "      for j in range(samplesize):\n",
        "      # forward propagation\n",
        "        output = x_train[j]\n",
        "        for layer in self.layers:\n",
        "          output = layer.forward_propagation(output)\n",
        "          # compute loss (for display purpose only)\n",
        "          errorval += self.loss(y_train[j], output)\n",
        "          # backward propagation\n",
        "          error = self.loss_prime(y_train[j], output)\n",
        "          for layer in reversed(self.layers):\n",
        "            error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "      # calculate average error on all samples\n",
        "      errorval /= samplesize\n",
        "      print('epoch %d/%d   error=%f' % (i+1, epochs, errorval))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "whtyud9TtQxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1=linearlayer(5,4)\n",
        "print(model1.output)\n",
        "#model=Layer()\n",
        "print(\"weights: \", model1.weights)\n",
        "print(\"bias: \",model1.bias)"
      ],
      "metadata": {
        "id": "BYBtmUr1f2bz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}